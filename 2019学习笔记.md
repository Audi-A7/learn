# 2019学习笔记

标题
==

标签（空格分隔）： presto hive mongDB 响应式编程 spring的状态机 缓存相关

---

presto参考文档：

 1. https://tech.meituan.com/2014/06/16/presto.html
 2. http://www.stay-stupid.com/?p=395
 

hive参考文档：

 1. https://blog.csdn.net/wangyang1354/article/details/50570903
 2. https://blog.csdn.net/LW_GHY/article/details/51469753
 3. https://www.jianshu.com/p/dbad3b3d40eb

MongoDB参考文档：

 1. https://www.w3cschool.cn/mongodb/mongodb-query.html
 

spring5的webflux参考资料：

 1. https://juejin.im/post/5b3a22a16fb9a024db5ff13e
 2. https://zhuanlan.zhihu.com/p/37846655
 3. https://github.com/pkpk1234/learn-reactor
 

spring的状态机参考资料：

 1. http://blog.didispace.com/spring-statemachine/
 


缓存相关参考资料：

2. http://zhuanlan.51cto.com/art/201806/577116.htm
3. https://tech.meituan.com/2017/03/17/cache-about.html


poi生成pdf文档：

 1. https://blog.csdn.net/makang456/article/details/70161037
 

mysql间歇锁
--------

https://blog.csdn.net/andyxm/article/details/44810417
https://zhuanlan.zhihu.com/p/48269420
https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html#innodb-gap-locks

需要强调一下：间隙锁在**主键索引、普通索引**上的性质略微有些差别。

所谓间隙锁，区别于行锁只锁住一行，间隙锁会锁住一个区间段的数据。

下面先讨论**主键索引**上的间隙锁：

比如：
```mysql
CREATE TABLE `test` (
  `id` int(1) NOT NULL AUTO_INCREMENT,
  `name` varchar(8) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

INSERT INTO `test` VALUES ('1', '小罗');
INSERT INTO `test` VALUES ('5', '小黄');
INSERT INTO `test` VALUES ('7', '小明');
INSERT INTO `test` VALUES ('11', '小红');
```
上面数据存在隐藏的间隙锁：

 1. (-infinity, 1]
 2. (1, 5]
 3. (5, 7]
 4. (7, 11]
 5. (11, +infinity]

然后执行如下的sql语句，会产生间隙锁。
```mysql
/* 开启事务1 */
BEGIN;
/* 查询 id 在 5 - 7 范围的数据并加记录锁 */
SELECT * FROM `test` WHERE `id` BETWEEN 5 AND 7 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句

/* 事务2插入一条 id = 3，name = '小张1' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (3, '小张1'); # 正常执行

/* 事务3插入一条 id = 4，name = '小白' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (4, '小白'); # 正常执行

/* 事务4插入一条 id = 6，name = '小东' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (6, '小东'); # 阻塞

/* 事务5插入一条 id = 8， name = '大罗' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (8, '大罗'); # 阻塞

/* 事务6插入一条 id = 9， name = '大东' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (9, '大东'); # 阻塞

/* 事务7插入一条 id = 11， name = '李西' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (11, '李西'); # 阻塞

/* 事务8插入一条 id = 12， name = '张三' 的数据 */
INSERT INTO `test` (`id`, `name`) VALUES (12, '张三'); # 正常执行

/* 提交事务1，释放事务1的锁 */
COMMIT;
```
上面的for update语句锁住了[5-7]、[7-11]**两个区间**的数据！！！

另外。如果for update语句或者delete语句操作的数据在数据库不存在，那么也会产生间隙锁，但是它们只会锁住紧邻的区间。比如，删除id=3的数据（忽略上面事务的sql产生的数据），那么会产生[1-5]的间歇锁。

下面再看一下**普通索引**上的间隙锁：
准备如下表结构及初始数据：
```mysql
# 注意：number 不是唯一值

CREATE TABLE `test1` (
  `id` int(1) NOT NULL AUTO_INCREMENT,
  `number` int(1) NOT NULL COMMENT '数字',
  PRIMARY KEY (`id`),
  KEY `number` (`number`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;

# 初始化数据
INSERT INTO `test1` VALUES (1, 1);
INSERT INTO `test1` VALUES (5, 3);
INSERT INTO `test1` VALUES (7, 8);
INSERT INTO `test1` VALUES (11, 12);
```
 number 索引存在的隐藏间隙：
 
 1. (-infinity, 1]
 2. (1, 3]
 3. (3, 8]
 4. (8, 12]
 5. (12, +infinity]

然后执行如下的事务：
```mysql
/* 开启事务1 */
BEGIN;
/* 查询 number = 2 的数据并加记录锁 */
SELECT * FROM `test1` WHERE `number` = 2 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句

/* 事务2插入一条 number = 0 的数据 */
INSERT INTO `test1` (`number`) VALUES (0); # 正常执行

/* 事务3插入一条 number = 1 的数据 */
INSERT INTO `test1` (`number`) VALUES (1); # 被阻塞

/* 事务4插入一条 number = 2 的数据 */
INSERT INTO `test1` (`number`) VALUES (2); # 被阻塞

/* 事务5插入一条 number = 4 的数据 */
INSERT INTO `test1` (`number`) VALUES (4); # 正常执行

/* 事务6插入一条 number = 8 的数据 */
INSERT INTO `test1` (`number`) VALUES (8); # 正常执行

/* 事务7插入一条 number = 9 的数据 */
INSERT INTO `test1` (`number`) VALUES (9); # 正常执行

/* 事务8插入一条 number = 10 的数据 */
INSERT INTO `test1` (`number`) VALUES (10); # 正常执行

/* 提交事务1 */
COMMIT;
```

可以看到，上面的sql产生number索引了[1-3]的间隙锁。

再看下下面的sql：
```mysql
/* 开启事务1 */
BEGIN;
/* 查询 number = 5 的数据并加记录锁 */
SELECT * FROM `test1` WHERE `number` = 3 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

# 注意：以下的语句不是放在一个事务中执行，而是分开多次执行，每次事务中只有一条添加语句

/* 事务2插入一条 number = 0 的数据 */
INSERT INTO `test1` (`number`) VALUES (0); # 正常执行

/* 事务3插入一条 number = 1 的数据 */
INSERT INTO `test1` (`number`) VALUES (1); # 被阻塞

/* 事务4插入一条 number = 2 的数据 */
INSERT INTO `test1` (`number`) VALUES (2); # 被阻塞

/* 事务5插入一条 number = 4 的数据 */
INSERT INTO `test1` (`number`) VALUES (4); # 被阻塞

/* 事务6插入一条 number = 8 的数据 */
INSERT INTO `test1` (`number`) VALUES (8); # 正常执行

/* 事务7插入一条 number = 9 的数据 */
INSERT INTO `test1` (`number`) VALUES (9); # 正常执行

/* 事务8插入一条 number = 10 的数据 */
INSERT INTO `test1` (`number`) VALUES (10); # 正常执行

/* 提交事务1 */
COMMIT;
```
上面的sql，即便存在number=3的记录，但是还是产生了[1-8]的间隙锁。至于原因，我们先看一下下面的sql再解释:
```java
/* 开启事务1 */
BEGIN;
/* 查询 number = 5 的数据并加记录锁 */
SELECT * FROM `test1` WHERE `number` = 3 FOR UPDATE;
/* 延迟30秒执行，防止锁释放 */
SELECT SLEEP(30);

/* 事务1插入一条 id = 2， number = 1 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (2, 1); # 阻塞

/* 事务2插入一条 id = 3， number = 2 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (3, 2); # 阻塞

/* 事务3插入一条 id = 6， number = 8 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (6, 8); # 阻塞

/* 事务4插入一条 id = 8， number = 8 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (8, 8); # 正常执行

/* 事务5插入一条 id = 9， number = 9 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (9, 9); # 正常执行

/* 事务6插入一条 id = 10， number = 12 的数据 */
INSERT INTO `test1` (`id`, `number`) VALUES (10, 12); # 正常执行

/* 事务7修改 id = 11， number = 12 的数据 */
UPDATE `test1` SET `number` = 5 WHERE `id` = 11 AND `number` = 12; # 阻塞

/* 提交事务1 */
COMMIT;
```
注意上面的事务3和事务4.事务3阻塞了，事务4却成功了。这是为啥？看看下面这张图：
![此处输入图片的描述][1]
如上图所示，存在原始数据id=7，number=8，因此事务4的id=8，number=8不会被阻塞。

 1. 在普通索引列上，**不管**是何种查询，只要加锁，都会产生间隙锁，这跟唯一索引不一样；
 2. 在普通索引跟唯一索引中，数据间隙的分析，数据行是**优先**根据**普通索引**排序，再根据唯一索引排序。
 4. 临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。临键锁的主要目的，也是为了避免幻读(Phantom Read)。如果把事务的隔离级别降级为RC，临键锁则也会失效。


Java的泛型
-------
https://www.toutiao.com/i6728925616785080844/

Java 泛型（generics）是 JDK 5 中引入的一个新特性,泛型提供了编译时类型安全检测机制，该机制允许开发者在编译时检测到非法的类型。泛型的好处就是在编译的时候能够检查类型安全，并且所有的强制转换都是自动和隐式的。

我们在定义泛型类，泛型方法，泛型接口的时候经常会碰见很多不同的通配符，比如 T，E，K，V 等等，这些通配符又都是什么意思呢？

本质上这些个都是通配符，没啥区别，只不过是编码时的一种约定俗成的东西。比如上述代码中的 T ，我们可以换成 A-Z 之间的任何一个 字母都可以，并不会影响程序的正常运行，但是如果换成其他的字母代替 T ，在可读性上可能会弱一些。通常情况下，T，E，K，V，？ 是这样约定的：

 - ？ 表示不确定的 java 类型
 - T (type) 表示具体的一个java类型
 - K V (key value) 分别代表java键值中的Key Value
 - E (element) 代表Element

首先说一下**?**通配符：
![此处输入图片的描述][2]
当调用 countLegs1 时，就会飘红，提示的错误信息如下：
![此处输入图片的描述][3]

所以，对于不确定或者不关心实际要操作的类型，可以使用无限制通配符（尖括号里一个问号，即 <?> ），表示可以持有任何类型。像 countLegs 方法中，限定了**上界**，但是不关心具体类型是什么，所以对于传入的 **Animal** 的所有**子类**都可以支持，并且不会报错。而 countLegs1 就不行。

拼多多面经
-----

https://www.toutiao.com/i6673066864022651396/

tcc事务学习  2pc 3pc 比较
-------

 1. https://blog.csdn.net/u013380694/article/details/83347764
 2. https://www.cnblogs.com/jajian/p/10014145.html
 3. https://juejin.im/post/5bf201f7f265da610f63528a

tcc（try-confirm-Cancel）事务这个东西，个人觉得其实就是一个类似于2pc的分布式事务处理机制。为了达到某种操作（例如商品订单的业务），它并不会直接进行操作，而是有一个预操作的过程（try），如果try成功了，那么才会进行confirm（注意：tcc有一个隐含的设定，try成功以后，那么必须confirm也成功，就好比我们订火车票一样，一旦锁定席位成功，在剩余的15分钟内，只要我们付款，那么一定可以保证出票成功，一样的道理）。如果try失败了，那么才会有cancel这一步。

连接3里使用我们平时都会接触到的订单的业务来讲解，通俗易懂。

一般tcc框架，除非大公司，一般公司都会使用一些开源的框架比如

 1. https://github.com/liuyangming/ByteTCC

ByteTCC可以方便的与springcloud或者dubbo集成。
![此处输入图片的描述][4]
 
 

springcloud部分组件底层原理
-------------------

 
 

springcloud参数调优
---------------

 1. https://juejin.im/post/5be83e166fb9a049a7115580

其实，就如参考文档中说的一样，一般的调优主要分成下面几个方面

 - 数据库做读写分离（甚至分库分表）
 - 调整服务之间调用的超时时间（当然这个也不能太大，否则会导致容器的连接池被占满，从而hang住）
 - 数据库合理配置索引，且不要写「大」sql，复杂的业务处理最好放在java代码中进行，一时降低数据库压力，二是方便后期维护（看java代码总比看sql好吧。。）
 - 配置ribbon的自动重试次数（一般失败后自动重试一次,如下图所示），并且需要进行接口幂等性设置，防止重刷
 - 使用缓存，对于非频繁变化的数据进行缓存，合理设置过期时间（对于redis的key建议参考阿里的redis规范进行设置）

 ![此处输入图片的描述][5]
 

面试经典面经
----

 1. https://juejin.im/post/5d6f0987f265da03cf7aab4f


腾讯面经
----

 1. https://www.toutiao.com/i6628527382590390798/

DispatcherServlet执行流程
---------------------

 1. https://www.jianshu.com/p/0f981efdfbbd
 2. https://www.cnblogs.com/tengyunhao/p/7518481.html

DispatcherServlet执行流程，其实简单来说，无非就是根据url定位到可以处理该请求的controller方法上，然后进行相关处理，然后返回。

详细一点的话就是，DispatcherServlet通过HandlerMapping定位具体的controller类，并且返回HandlerExecutionChain对象，然后根据HandlerExecutionChain定位到具体的执行方法，也就是HandlerAdapter，最后执行完成进行返回。

更详细的可以参考链接1：

![此处输入图片的描述][6]

具体处理过程如下:

1、用户请求发送至DispatcherServlet类进行处理。

2、DispatcherServlet类遍历所有配置的HandlerMapping类请求查找Handler。

3、HandlerMapping类根据request请求的URL等信息查找能够进行处理的Handler，以及相关拦截器interceptor并构造HandlerExecutionChain。

4、HandlerMapping类将构造的HandlerExecutionChain类的对象返回给前端控制器DispatcherServlet类。

5、前端控制器拿着上一步的Handler遍历所有配置的HandlerAdapter类请求执行Handler。

6、HandlerAdapter类执行相关Handler并获取ModelAndView类的对象。

7、HandlerAdapter类将上一步Handler执行结果的ModelAndView 类的对象返回给前端控制器。

8、DispatcherServlet类遍历所有配置的ViewResolver类请求进行视图解析。

9、ViewResolver类进行视图解析并获取View对象。

10、ViewResolver类向前端控制器返回上一步骤的View对象。

11、DispatcherServlet类进行视图View的渲染，填充Model。

12、DispatcherServlet类向用户返回响应。

主流Java数据库连接池分析
--------------

 1. https://www.toutiao.com/i6551532416505217539/

常用的主流开源数据库连接池有C3P0、DBCP、Tomcat Jdbc Pool、BoneCP、Druid等。

**C3p0**: 开源的JDBC连接池，实现了数据源和JNDI绑定，支持JDBC3规范和JDBC2的标准扩展。目前使用它的开源项目有Hibernate、Spring等。单线程，性能较差，适用于小型系统，代码600KB左右。

**DBCP (Database Connection Pool)**:由Apache开发的一个Java数据库连接池项目， Jakarta commons-pool对象池机制，Tomcat使用的连接池组件就是DBCP。单独使用dbcp需要3个包：common-dbcp.jar,common-pool.jar,common-collections.jar，预先将数据库连接放在内存中，应用程序需要建立数据库连接时直接到连接池中申请一个就行，用完再放回。单线程，并发量低，性能不好，适用于小型系统。

**Tomcat Jdbc Pool**：Tomcat在7.0以前都是使用common-dbcp做为连接池组件，但是dbcp是单线程，为保证线程安全会锁整个连接池，性能较差，dbcp有超过60个类，也相对复杂。Tomcat从7.0开始引入了新增连接池模块叫做Tomcat jdbc pool，基于Tomcat JULI，使用Tomcat日志框架，完全兼容dbcp，通过异步方式获取连接，支持高并发应用环境，超级简单核心文件只有8个，支持JMX，支持XA Connection。

**BoneCP**：官方说法BoneCP是一个高效、免费、开源的Java数据库连接池实现库。设计初衷就是为了提高数据库连接池性能，根据某些测试数据显示，BoneCP的速度是最快的，要比当时第二快速的连接池快25倍左右，完美集成到一些持久化产品如Hibernate和DataNucleus中。BoneCP特色：高度可扩展，快速；连接状态切换的回调机制；允许直接访问连接；自动化重置能力；JMX支持；懒加载能力；支持XML和属性文件配置方式；较好的Java代码组织，100%单元测试分支代码覆盖率；代码40KB左右。

**Druid**：Druid是Java语言中最好的数据库连接池，Druid能够提供强大的监控和扩展功能，是一个可用于大数据实时查询和分析的高容错、高性能的开源分布式系统，尤其是当发生代码部署、机器故障以及其他产品系统遇到宕机等情况时，Druid仍能够保持100%正常运行。主要特色：为分析监控设计；快速的交互式查询；高可用；可扩展；Druid是一个开源项目，源码托管在github上。

主流连接池各项功能对比如下：

![此处输入图片的描述][7]

HikariCP与其他数据库连接池的对比：

![此处输入图片的描述][8]

**HikariCP性能分析：**

 1. HikariCP通过优化(concurrentBag，fastStatementList )集合来提高并发的读写效率。
 2. HikariCP使用threadlocal缓存连接及大量使用CAS的机制，最大限度的避免lock。但可能带来cpu使用率的上升（因为cas会有while循环）。
 3. 从字节码的维度优化代码。 (default inline threshold for a JVM running the server
    Hotspot compiler is 35 bytecodes ）让方法尽量在35个字节码一下，来提升jvm的处理效率。

 

OOM的常见情况
--------

 1. https://monkeysayhi.github.io/2018/11/05/Java%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81OOM%E5%8F%8A%E5%8E%9F%E5%9B%A0/
 2. https://www.jianshu.com/p/2fdee831ed03

**堆内存溢出**

这应该是我们最为常见的一种OOM，当堆内没有足够的内存供申请对象使用时，就会出现，示例代码如下：
```java
  public static void main(String args[]) throws Exception {
        UserController userController = new UserController();
        List list = new ArrayList();
        while (true) {
            list.add(userController);
        }
    }
```

运行输出：
```java
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3210)
	at java.util.Arrays.copyOf(Arrays.java:3181)
	at java.util.ArrayList.grow(ArrayList.java:265)
	at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:239)
	at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:231)
	at java.util.ArrayList.add(ArrayList.java:462)
	at com.t4f.gaea.sample.controller.UserNormalController.main(UserNormalController.java:47)
```

**java.lang.OutOfMemoryError:GC overhead limit exceeded**

当应用程序花费超过98%的时间用来做GC并且回收了不到2%的堆内存时，会抛出java.lang.OutOfMemoryError:GC overhead limit exceeded错误。具体的表现就是你的应用几乎耗尽所有可用内存，并且GC多次均未能清理干净。示例代码如下：
```java
 public static void main(String args[]) throws Exception {
        Map map = new HashMap();
        Random r = new Random();
        while (true) {
            map.put(r.nextInt(), "value");
        }
    }
```

注意设置JVM参数：
```java
-Xmx10m -XX:+UseParallelGC
```
运行输出：
```java
Exception in thread "main" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.lang.Integer.valueOf(Integer.java:832)
	at com.t4f.gaea.sample.controller.UserNormalController.main(UserNormalController.java:48)
```
 需要注意的是 ，上面的错误信息跟GC算法有很大关系，如果是以G1收集器的话，那么输出如下：
 ```java
 Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
	at java.util.HashMap.newNode(HashMap.java:1750)
	at java.util.HashMap.putVal(HashMap.java:631)
	at java.util.HashMap.put(HashMap.java:612)
	at com.t4f.gaea.sample.controller.UserNormalController.main(UserNormalController.java:48)
 ```
 
 **java.lang.OutOfMemoryError:Metaspace**
 
 元数据区溢出，示例代码如下：
 ```java
     public static Class generate(String name) throws Exception {
        ClassPool pool = ClassPool.getDefault();
        return pool.makeClass(name).toClass();
    }

    public static void main(String args[]) throws Exception {
        for (int i = 0; ; i++) {
            generate("com.audi" + i);
        }
    }
 ```
 需要设置虚拟机参数，-XX：MaxMetaspaceSize = 32m，否则可能元数据区在内存中，可能内存还没耗完，堆内存就先溢出了。
 
 输出如下：
 ```java
Exception in thread "main" org.apache.ibatis.javassist.CannotCompileException: by java.lang.OutOfMemoryError: Compressed class space
	at org.apache.ibatis.javassist.util.proxy.DefineClassHelper.toClass2(DefineClassHelper.java:140)
	at org.apache.ibatis.javassist.util.proxy.DefineClassHelper.toClass(DefineClassHelper.java:95)
	at org.apache.ibatis.javassist.ClassPool.toClass(ClassPool.java:1143)
	at org.apache.ibatis.javassist.ClassPool.toClass(ClassPool.java:1106)
 ```
 
 **栈溢出**
 递归调用，如果没有break的话，最容易出现栈溢出，示例代码如下：
 ```java
    
    private static void rec(){
        rec();
        return;
   }

    public static void main(String args[]) throws Exception {
        for (int i = 0; ; i++) {
            rec();
        }
    }
 ```
 

Redis的缓存过期策略和内存淘汰策略
-------------------
https://www.jianshu.com/p/8aa619933ebb

Redis的缓存过期策略和内存淘汰策略，是两个完全不同的东西，**过期策略**讲的是缓存key到期了，redis该如何处理这些数据；而**淘汰策略**讲的是redis内存不足时，redis该如何为新插入的数据申请内存空间。

 - Redis的过期策略

过期策略通常有以下三种：

**定时过期**：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。

**惰性过期**：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。

**定期过期**：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。
(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)

Redis中同时使用了**惰性**过期和**定期**过期两种过期策略。

 - Redis的内存淘汰策略

Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。

 1. noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。
 2. allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。
 3. allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。
 4. volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。
 5. volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。
 6. volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。

rpc通信和http通信的区别联系
-----------------

 1. https://www.zhihu.com/question/41609070
 2. https://www.cnblogs.com/winner-0715/p/5847638.html
 3. https://blog.csdn.net/MOU_IT/article/details/79873612

个人总结来看，其实就是rpc协议相对来说，更加灵活，它可以直接建立在tcp之上，也可以建立在http协议之上。但是http不行，它必须遵循特定的规范，且每次请求附加的冗余信息较多。比如下面是一个典型的http响应报文格式：
```html
HTTP/1.0 200 OK 
Content-Type: text/plain
Content-Length: 137582
Expires: Thu, 05 Dec 1997 16:00:00 GMT
Last-Modified: Wed, 5 August 1996 15:55:28 GMT
Server: Apache 0.84

<html>
  <body>Hello World</body>
</html>
```
与之对比，由于rpc可以自定义交互格式，假如定义成如下格式：
![此处输入图片的描述][9]

可以明显看出，在一次交互过程中，rpc传输的冗余信息更少。在带宽一定的条件下，效率相对会更高（这一般都是针对http1.1而言）。

http2.0版本已经针对http协议本身进行了大幅的优化，比如支持并发请求服务器，下面会进行介绍。

http2.0协议
---------

 1. https://www.thewebmaster.com/hosting/2015/dec/14/what-is-http2-and-how-does-it-compare-to-http1-1/#targetText=HTTP1.x%20uses%20text%2Dbased%20commands%20to%20complete%20HTTP%20requests.&targetText=HTTP2%2C%20on%20the%20other%20hand,0s)%20to%20complete%20HTTP%20requests.
 2. 

HTTP2相对于http1.1而言，主要有以下一些优势（引用链接1的部分内容）。

The key differences HTTP/2 has to HTTP/1.x are as follows:

 1. It is binary, instead of textual
 2. It is fully multiplexed, instead of ordered and blocking
 3. It can use one connection for parallelism
 4. It uses header compression to reduce overhead
 5. It allows Server Pushing to add responses proactively into the
    Browser cache.

整体来说，其实就是是用了二进制协议进行数据帧的传输，使用了分帧技术进行数据传输，得益于使用了数据分帧技术，使得多个请求可以同时发送，而不会阻塞。并且，http2具有数据优先级的概念，使得例如css信息，js依赖库可以优先返回。并且给予server push技术，服务器可以主动向客户端推送信息。

下图展示了数据的分帧过程：
![此处输入图片的描述][10]

下图展示了数据优先级的过程：
![此处输入图片的描述][11]

需要注意的是，一个http2链接的并发数量也是有一定限制，如果超出，那么还是需要新家http链接才可以。不同浏览器的规定如下：
|Browser|	Max Parallel Connections Per Host|
| --------   | -----  |
|IE 9	|6
|IE 10	|8
|Firefox 4+	|6|
|Opera 11+|	6|
|Chrome 4+|	6|
|Safari|	4|

下面详细说一下server push，主要通过有和没有server push的请求过程来说明，首先是没有server push的过程：

Page Load without Server Push
A server without Server Push will follow a simple process:

 1. An HTTP Request is made for the HTML file.
 2. The Server provides the HTML file in response.
 3. The HTML document references a CSS file, JavaScript file, and maybe
    some images. Client requests are then made for those resources,
    which are then returned by the server.
 4. Once the resources are returned the browser renders the page.

You can see this process visualized below:
![此处输入图片的描述][12]

The problem with this method is that it takes time to discover the assets in the HTML page, and then more time to retrieve them. This delays the rendering of the web page and increases web page load times.

然后是有server push的过程：

Page load with Server Push
A server with Server Push will follow an even more straightforward process:

 1. The HTTP Request is made for the HTML file.
 2. The Server provides the HTML file in response, along with the CSS
    file.
 3. **The page starts to render straight away.**
 4. Afterward, other less critical assets can be retrieved, such as the
    JavaScript file, and images.

You can see this process visualized below:

![此处输入图片的描述][13]
server push需要慎重使用，因为这会加重服务器的负载：

The best practice **is not to** push all your assets; just the ones that hold up the page from rendering. If you push too many resources at once it can cause your site to be slower to load, so be careful.


 - 下面简要介绍一下**http3**

http3最大的区别是，给予udp协议，而不再是tcp。

HTTP/3 is an evolution of the QUIC (Quick UDP Internet Connections) protocol from Google, first suggested by Mark Nottingham in October 2018. HTTP/3 is due to be released in 2019 (hopefully).

QUIC is similar to TCP+TLS+HTTP2 but is implemented on top of UDP. UDP stands for User Datagram Protocol. UDP is essentially TCP without all the error checking.

This has many benefits:

 - UDP packets are received by the recipient more quickly.
 - The sender will not have to wait to ensure the packet has been
   received.

Because no error checks are made, when the recipient misses packets, they cannot be requested again. As a result, UDP is used when speed is more important than the occasional lossed packet, such as live broadcasts or online gaming.

Key features of QUIC:

 - Dramatically reduced connection establishment time
 - Improved congestion control
 - Multiplexing without head-of-line blocking
 - Forward error correction
 - Connection migration

 
 
二进制协议与文本协议
----------


 

java的序列化与反序列化
-------------

 1. https://www.hollischuang.com/archives/1140
 2. https://blog.chaitin.cn/2015-11-11_java_unserialize_rce/

首先说一下定义：

 - 序列化：将java对象的属性值转换为流的形式，保存在磁盘或进行网络传输的过程叫序列化（对象序列化**不关注类中的静态变量**）。
 - 反序列化：将数据从流中读出，转换为对象的过程叫反序列化。

下面稍微修改链接1的例子，演示一下序列化，反序列化的过程：

首先是新建一个User对象，实现Serializable接口：
```java
package com.hollis;
import java.io.Serializable;
import java.util.Date;

/**
 * Created by hollis on 16/2/2.
 */
public class User implements Serializable{
    private String name;
    private int age;
    private static String country;
    private Date birthday;
    private transient String gender;
    private static final long serialVersionUID = -6849794470754667710L;

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getAge() {
        return age;
    }

    public void setAge(int age) {
        this.age = age;
    }

    public Date getBirthday() {
        return birthday;
    }

    public void setBirthday(Date birthday) {
        this.birthday = birthday;
    }

    public String getGender() {
        return gender;
    }

    public void setGender(String gender) {
        this.gender = gender;
    }

    @Override
    public String toString() {
        return "User{" +
                "name='" + name + '\'' +
                ", age=" + age +
                ", gender=" + gender +
                ", birthday=" + birthday +
                '}';
    }
}
```
 然后是对User进行序列化：
 ```java
 import org.apache.commons.io.IOUtils;

import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectOutputStream;
import java.util.Date;

/**
 * Created by hollis on 16/2/2.
 */
public class SerializableDemo {

    public static void main(String[] args) {
        //Initializes The Object
        User user = new User();
        user.setName("hollis");
        user.setGender("male");
        user.setAge(23);
        user.setGender("Male");
        user.setBirthday(new Date());
        User.setCountry("China");
        System.out.println(user);

        //Write Obj to File
        ObjectOutputStream oos = null;
        try {
            oos = new ObjectOutputStream(new FileOutputStream("tempFile"));
            oos.writeObject(user);
        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeQuietly(oos);
        }

    }
}
// User{name='hollis', age=23, gender=Male, birthday=Wed Oct 02 11:59:40 CST 2019, country=China}
 ```
 
 执行完序列化代码以后，再执行如下的反序列化代码：
 ```java
 import org.apache.commons.io.FileUtils;
import org.apache.commons.io.IOUtils;

import java.io.*;
import java.util.Date;

/**
 * Created by hollis on 16/2/2.
 */
public class SerializableDemo2 {

    public static void main(String[] args) {

        //Read Obj from File
        File file = new File("tempFile");
        ObjectInputStream ois = null;
        try {
            ois = new ObjectInputStream(new FileInputStream(file));
            User newUser = (User) ois.readObject();
            System.out.println(newUser);
        } catch (IOException e) {
            e.printStackTrace();
        } catch (ClassNotFoundException e) {
            e.printStackTrace();
        } finally {
            IOUtils.closeQuietly(ois);
            try {
                FileUtils.forceDelete(file);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

    }
}
// User{name='hollis', age=23, gender=null, birthday=Wed Oct 02 11:59:40 CST 2019, country=null}
 ```
 
可以看出，gender和country都没有被序列化，也就是transient和static修饰的都无法被序列化。

**序列化及反序列化相关知识**

1、在Java中，只要一个类实现了java.io.Serializable接口，那么它就可以被序列化。

2、通过ObjectOutputStream和ObjectInputStream对对象进行序列化及反序列化

3、虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的**序列化 ID** 是否一致（就是 private static final long serialVersionUID）

4、序列化并不保存静态变量。

5、要想将父类对象也序列化，就需要让父类也实现Serializable 接口。

6、Transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。

7、服务器端给客户端发送序列化对象数据，对象中有一些数据是敏感的，比如密码字符串等，希望对该密码字段在序列化时，进行加密，而客户端如果拥有解密的密钥，只有在客户端进行反序列化时，才可以对密码进行读取，这样可以一定程度保证序列化对象的数据安全。
 
 

ArrayList的序列化（HashMap序列化类似）
-------------
ArrayList的部分定义：

```java
public class ArrayList<E> extends AbstractList<E>
        implements List<E>, RandomAccess, Cloneable, java.io.Serializable
{
    private static final long serialVersionUID = 8683452581122892189L;
    transient Object[] elementData; // non-private to simplify nested class access
    private int size;
}
```

从上面的代码中可以知道ArrayList实现了java.io.Serializable接口，那么我们就可以对它进行序列化及反序列化。因为elementData是transient的，所以我们认为这个成员变量不会被序列化而保留下来。我们写一个Demo，验证一下我们的想法：
```java
public static void main(String[] args) throws IOException, ClassNotFoundException {
        List<String> stringList = new ArrayList<String>();
        stringList.add("hello");
        stringList.add("world");
        stringList.add("hollis");
        stringList.add("chuang");
        System.out.println("init StringList" + stringList);
        ObjectOutputStream objectOutputStream = new ObjectOutputStream(new FileOutputStream("stringlist"));
        objectOutputStream.writeObject(stringList);

        IOUtils.close(objectOutputStream);
        File file = new File("stringlist");
        ObjectInputStream objectInputStream = new ObjectInputStream(new FileInputStream(file));
        List<String> newStringList = (List<String>)objectInputStream.readObject();
        IOUtils.close(objectInputStream);
        if(file.exists()){
            file.delete();
        }
        System.out.println("new StringList" + newStringList);
    }
//init StringList[hello, world, hollis, chuang]
//new StringList[hello, world, hollis, chuang]
```

了解ArrayList的人都知道，ArrayList底层是通过数组实现的。那么数组elementData其实就是用来保存列表中的元素的。通过该属性的声明方式我们知道，他是无法通过序列化持久化下来的。那么为什么code 4的结果却通过序列化和反序列化把List中的元素保留下来了呢？

**writeObject和readObject方法**

在ArrayList中定义了来个方法： writeObject和readObject。

具体看一下两个方法是实现：
```java
private void readObject(java.io.ObjectInputStream s)
        throws java.io.IOException, ClassNotFoundException {
        elementData = EMPTY_ELEMENTDATA;

        // Read in size, and any hidden stuff
        s.defaultReadObject();

        // Read in capacity
        s.readInt(); // ignored

        if (size > 0) {
            // be like clone(), allocate array based upon size not capacity
            ensureCapacityInternal(size);

            Object[] a = elementData;
            // Read in all elements in the proper order.
            for (int i=0; i<size; i++) {
                a[i] = s.readObject();
            }
        }
    }



private void writeObject(java.io.ObjectOutputStream s)
        throws java.io.IOException{
        // Write out element count, and any hidden stuff
        int expectedModCount = modCount;
        s.defaultWriteObject();

        // Write out size as capacity for behavioural compatibility with clone()
        s.writeInt(size);

        // Write out all elements in the proper order.
        for (int i=0; i<size; i++) {
            s.writeObject(elementData[i]);
        }

        if (modCount != expectedModCount) {
            throw new ConcurrentModificationException();
        }
    }
```

为什么ArrayList要将数组元素设置为transiet，通过实现writeObject和readObject来进行序列化操作呢？
 
 ArrayList实际上是动态数组，每次在放满以后自动增长设定的长度值，如果数组自动增长长度设为100，而实际只放了一个元素，那就会序列化99个null元素。为了保证在序列化的时候不会将这么多null同时进行序列化，ArrayList把元素数组设置为transient。
 
 虽然ArrayList中写了writeObject 和 readObject 方法，但是这两个方法并没有显示的被调用啊。

那么如果一个类中包含writeObject 和 readObject 方法，那么这两个方法是怎么被调用的呢?

对象的序列化过程通过ObjectOutputStream和ObjectInputputStream来实现的，那么带着刚刚的问题，我们来分析一下ArrayList中的writeObject 和 readObject 方法到底是如何被调用的呢？

为了节省篇幅，这里给出ObjectOutputStream的writeObject的调用栈：

writeObject ---> writeObject0 --->writeOrdinaryObject--->writeSerialData--->invokeWriteObject

这里看一下invokeWriteObject：
```java
void invokeWriteObject(Object obj, ObjectOutputStream out)
        throws IOException, UnsupportedOperationException
    {
        if (writeObjectMethod != null) {
            try {
                writeObjectMethod.invoke(obj, new Object[]{ out });
            } catch (InvocationTargetException ex) {
                Throwable th = ex.getTargetException();
                if (th instanceof IOException) {
                    throw (IOException) th;
                } else {
                    throwMiscException(th);
                }
            } catch (IllegalAccessException ex) {
                // should not occur, as access checks have been suppressed
                throw new InternalError(ex);
            }
        } else {
            throw new UnsupportedOperationException();
        }
    }
```

其中writeObjectMethod.invoke(obj, new Object[]{ out });是关键，通过反射的方式调用writeObjectMethod方法。

如果一个类中包含writeObject 和 readObject 方法,在使用ObjectOutputStream的writeObject方法和ObjectInputStream的readObject方法时，会通过反射的方式调用。

**Serializable明明就是一个空的接口，它是怎么保证只有实现了该接口的方法才能进行序列化与反序列化的呢？**

Serializable接口的定义：
```java
public interface Serializable {
}
```
 
 其实这个问题也很好回答，我们再回到刚刚ObjectOutputStream的writeObject的调用栈：
 
 writeObject ---> writeObject0 --->writeOrdinaryObject--->writeSerialData--->invokeWriteObject
 
 writeObject0方法中有这么一段代码：
 ```java
 if (obj instanceof String) {
                writeString((String) obj, unshared);
            } else if (cl.isArray()) {
                writeArray(obj, desc, unshared);
            } else if (obj instanceof Enum) {
                writeEnum((Enum<?>) obj, desc, unshared);
            } else if (obj instanceof Serializable) {
                writeOrdinaryObject(obj, desc, unshared);
            } else {
                if (extendedDebugInfo) {
                    throw new NotSerializableException(
                        cl.getName() + "\n" + debugInfoStack.toString());
                } else {
                    throw new NotSerializableException(cl.getName());
                }
            }
```
在进行序列化操作时，会判断要被序列化的类是否是Enum、Array和Serializable类型，如果不是则直接抛出NotSerializableException。

**结论**

在序列化过程中，如果被序列化的类中定义了writeObject 和 readObject 方法，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化。

如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。

用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程，比如可以在序列化的过程中动态改变序列化的数值。

java序列化漏洞
----------

 1. https://blog.chaitin.cn/2015-11-11_java_unserialize_rce/
 2. https://www.ibm.com/developerworks/cn/java/j-5things1/


 java序列化的安全问题，个人觉得其实是针对反序列化来说的，因为序列化以后的二进制数据可以完整的被反序列化，这会导致某些敏感信息被泄露。例如，银行卡号，序列化以后，经过网络传输，如果传输过程中被被反序列化。。。
 
 
我们可以通过实现自己的writeObject  来进行敏感数据的编码， readObject进行解码。

例如，下面的代码对Person类的年龄进行编解码：

```java
public class Person
    implements java.io.Serializable
{
    public Person(String fn, String ln, int a)
    {
        this.firstName = fn; this.lastName = ln; this.age = a;
    }
 
    public String getFirstName() { return firstName; }
    public String getLastName() { return lastName; }
    public int getAge() { return age; }
    public Person getSpouse() { return spouse; }
     
    public void setFirstName(String value) { firstName = value; }
    public void setLastName(String value) { lastName = value; }
    public void setAge(int value) { age = value; }
    public void setSpouse(Person value) { spouse = value; }
 
    private void writeObject(java.io.ObjectOutputStream stream)
        throws java.io.IOException
    {
        // "Encrypt"/obscure the sensitive data
        age = age << 2;
        stream.defaultWriteObject();
    }
 
    private void readObject(java.io.ObjectInputStream stream)
        throws java.io.IOException, ClassNotFoundException
    {
        stream.defaultReadObject();
 
        // "Decrypt"/de-obscure the sensitive data
        age = age << 2;
    }
     
    public String toString()
    {
        return "[Person: firstName=" + firstName + 
            " lastName=" + lastName +
            " age=" + age +
            " spouse=" + (spouse!=null ? spouse.getFirstName() : "[null]") +
            "]";
    }      
 
    private String firstName;
    private String lastName;
    private int age;
    private Person spouse;
}
```

 初次之外，序列化的数据可以被**签名和密封**。
 
 通过使用 writeObject 和 readObject 可以实现密码加密和签名管理，但其实还有更好的方式。

如果需要对整个对象进行加密和签名，最简单的是将它放在一个 javax.crypto.SealedObject 和/或 java.security.SignedObject 包装器中。两者都是可序列化的，所以将对象包装在 SealedObject 中可以围绕原对象创建一种 “包装盒”。必须有对称密钥才能解密，而且密钥必须单独管理。同样，也可以将 SignedObject 用于数据验证，并且对称密钥也必须单独管理。

结合使用这两种对象，便可以轻松地对序列化数据进行密封和签名，而不必强调关于数字签名验证或加密的细节。

对于序列化的数据，反序列化时，如果需要验证，可以实现 ObjectInputValidation 接口，并覆盖 validateObject() 方法。如果调用该方法时发现某处有错误，则抛出一个 InvalidObjectException。
 

hession序列化对比Java序列化
----------
https://www.cnblogs.com/wzyxidian/p/5726584.html

**Java序列化：**

　　Java序列化会把要序列化的对象类的元数据和业务数据全部序列化为字节流，而且是把整个继承关系上的东西全部序列化了。它序列化出来的字节流是对那个对象结构到内容的完全描述，包含所有的信息，因此效率较低而且字节流比较大。但是由于确实是序列化了所有内容，所以可以说什么都可以传输，因此也更可用和可靠。


**hession序列化：**

　　它的实现机制是着重于数据，附带简单的类型信息的方法。就像Integer a = 1，hessian会序列化成I 1这样的流，I表示int or Integer，1就是数据内容。而对于复杂对象，通过Java的反射机制，hessian把对象所有的属性当成一个Map来序列化，产生类似M className propertyName1 I 1 propertyName S stringValue（大概如此，确切的忘了）这样的流，包含了基本的类型描述和数据内容。而在序列化过程中，如果一个对象之前出现过，hessian会直接插入一个R index这样的块来表示一个引用位置，从而省去再次序列化和反序列化的时间。这样做的代价就是hessian需要对不同的类型进行不同的处理（因此hessian直接偷懒不支持short），而且遇到某些特殊对象还要做特殊的处理（比如StackTraceElement）。而且同时因为并没有深入到实现内部去进行序列化，所以在某些场合会发生一定的不一致，比如通过Collections.synchronizedMap得到的map。
 
 
 
 
 
 


  [1]: https://github.com/WQZ321123/learn/blob/master/image/mysql/%E6%99%AE%E9%80%9A%E7%B4%A2%E5%BC%95%E7%9A%84%E9%97%B4%E9%9A%99%E9%94%81.jpg?raw=true
  [2]: https://github.com/Audi-A7/learn/blob/master/image/other/%E9%97%AE%E5%A5%BD%E9%80%9A%E9%85%8D%E7%AC%A6.jpg?raw=true
  [3]: https://github.com/Audi-A7/learn/blob/master/image/other/error.jpg?raw=true
  [4]: https://github.com/Audi-A7/learn/blob/master/image/2019/ByteTCC.png?raw=true
  [5]: https://github.com/Audi-A7/learn/blob/master/image/2019/ribbon_auto_retry.png?raw=true
  [6]: https://raw.githubusercontent.com/Audi-A7/learn/master/image/2019/dispatcherServlet.webp?token=ABXI2UIEHF4VBVTMDBAGLX25RIIJQ
  [7]: https://github.com/Audi-A7/learn/blob/master/image/2019/connector.jpg?raw=true
  [8]: https://github.com/Audi-A7/learn/blob/master/image/2019/connector2.jpg?raw=true
  [9]: https://github.com/Audi-A7/learn/blob/master/image/2019/rpc.jpg?raw=true
  [10]: https://raw.githubusercontent.com/Audi-A7/learn/master/image/2019/http2%20frame.webp?token=ABXI2UOURBKDKRA5B7VQ7NK5R362S
  [11]: https://raw.githubusercontent.com/Audi-A7/learn/master/image/2019/http2%20prority.webp?token=ABXI2UKI354CO4HMKMESPC25R37LI
  [12]: https://raw.githubusercontent.com/Audi-A7/learn/master/image/2019/no-server-push.webp?token=ABXI2UMF23I7B4AV4JT6F2C5R374U
  [13]: https://raw.githubusercontent.com/Audi-A7/learn/master/image/2019/server-push.webp?token=ABXI2ULPDY7LN5AU4TXVSFK5R4AE2